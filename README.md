# Covertype ML

Este proyecto contiene un pipeline de **Machine Learning** para el preprocesamiento, anÃ¡lisis y entrenamiento de modelos basado en **TensorFlow Data Validation (TFDV)** y **TensorFlow Transform (TFT)**.

---

## **CaracterÃ­sticas del Proyecto**

âœ… **Carga de datos** desde un archivo CSV.\
âœ… **AnÃ¡lisis exploratorio** con estadÃ­sticas y visualizaciÃ³n de datos.\
âœ… **DetecciÃ³n de anomalÃ­as** y validaciÃ³n del esquema de datos.\
âœ… **Preprocesamiento de datos** utilizando TensorFlow Transform.\
âœ… **DivisiÃ³n del dataset** en entrenamiento y prueba.\
âœ… **Almacenamiento de datos preprocesados**.

---

Este proyecto contiene un entorno de desarrollo para **Machine Learning** utilizando **Docker Compose**.  
El entorno incluye:

âœ… **JupyterLab** para la creaciÃ³n y entrenamiento de modelos.  
âœ… **FastAPI** para exponer los modelos como una API de predicciÃ³n.  
âœ… **Carpeta compartida** entre Jupyter y FastAPI para almacenar los modelos entrenados.  

---

## 1. Preprocesamiento de Datos
El pipeline de preprocesamiento se encarga de limpiar, transformar y dividir los datos antes de su uso en el modelo.

### Pasos del pipeline:
1. **Carga del dataset**: Se utiliza Pandas para leer el archivo CSV.
2. **GeneraciÃ³n de estadÃ­sticas**: Se usa `tensorflow_data_validation` (TFDV) para obtener estadÃ­sticas descriptivas de los datos.
3. **DivisiÃ³n de datos**: Se separa el dataset en conjuntos de entrenamiento (80%) y prueba (20%).
4. **Inferencia de esquema**: Se infiere la estructura de los datos con TFDV.
5. **DetecciÃ³n de anomalÃ­as**: Se validan los datos contra el esquema inferido para detectar inconsistencias.
6. **Transformaciones con `tensorflow_transform` (TFT)**: Se normalizan caracterÃ­sticas especÃ­ficas.
7. **Almacenamiento de los datos preprocesados**: Se guardan los conjuntos de datos transformados en archivos CSV.

## 2. Entrenamiento del Modelo
Se implementa un modelo de clasificaciÃ³n utilizando un **RandomForestClassifier** con preprocesamiento de caracterÃ­sticas y reducciÃ³n de dimensionalidad.

### Pasos del modelo:
1. **Carga del dataset**
2. **CodificaciÃ³n One-Hot para variables categÃ³ricas**
3. **EstandarizaciÃ³n de caracterÃ­sticas** con `StandardScaler`
4. **SelecciÃ³n de caracterÃ­sticas** con `SelectKBest`
5. **ReducciÃ³n de dimensionalidad** con PCA
6. **DivisiÃ³n de los datos en entrenamiento y prueba**
7. **Entrenamiento del modelo** con Random Forest
8. **EvaluaciÃ³n del modelo** (precisiÃ³n, recall, F1-score)
9. **Guardado del modelo y preprocesadores** con `joblib`

---

##  **Despliegue del Entorno**  

Para iniciar el entorno:  

```sh
docker compose up --build
```
Esto descargarÃ¡ las imÃ¡genes necesarias y levantarÃ¡ los contenedores de **JupyterLab** y **FastAPI**.  

Para detener los contenedores en cualquier momento, usa:  

```sh
docker compose down
```

Para reiniciar todo desde cero (eliminando volÃºmenes y datos), usa:  

```sh
docker compose down -v
```

---

## ğŸ“‚ **Estructura del Proyecto**  

```
ğŸ“‚ MLOps-Taller2
â”‚â”€â”€ ğŸ“‚ jupyter_work/              # Directorio donde se guardan los notebooks
â”‚â”€â”€ ğŸ“‚ models/                    # Carpeta compartida para almacenar los modelos entrenados
â”‚â”€â”€ ğŸ“‚ app/                        # Carpeta con la API FastAPI
â”‚   â”œâ”€â”€ ğŸ“‚ routes/                 # Endpoints de la API
â”‚   â”‚   â”œâ”€â”€ ğŸ“œ predict.py          # LÃ³gica de predicciÃ³n y carga de modelos
â”‚   â”œâ”€â”€ ğŸ“œ main.py                 # ConfiguraciÃ³n principal de FastAPI
â”‚â”€â”€ ğŸ“œ docker-compose.yml          # ConfiguraciÃ³n de los servicios en Docker
â”‚â”€â”€ ğŸ“œ Dockerfile                  # ConfiguraciÃ³n de la imagen para FastAPI
â”‚â”€â”€ ğŸ“œ requirements.txt            # Dependencias del proyecto
â”‚â”€â”€ ğŸ“œ README.md                   # DocumentaciÃ³n del proyecto
```

---

##  **Acceso a JupyterLab**  

Una vez que el contenedor estÃ¡ en ejecuciÃ³n, acceder a **JupyterLab** en:  

ğŸ”— **URL de acceso:**  
```
http://localhost:8888
```
El acceso estÃ¡ configurado para **no requerir token ni contraseÃ±a**.  

---


## **ConclusiÃ³n**

Este pipeline permite:\
âœ… Explorar y analizar datos con **TFDV**.\
âœ… Detectar anomalÃ­as y validar esquemas.\
âœ… Aplicar transformaciones con **TFT**.\
âœ… Entrenar y evaluar modelos de clasificaciÃ³n.\
âœ… Realizar predicciones sobre nuevos datos.

